{"filter":false,"title":"calculos.py","tooltip":"/calculos.py","undoManager":{"mark":70,"position":70,"stack":[[{"start":{"row":0,"column":0},"end":{"row":111,"column":0},"action":"remove","lines":["\"\"\"","Your module description","comentarios múltiples","\"\"\"","","#comentario de una línea","","from flask import Flask, render_template,request ,url_for","import numpy as np","import math","","documents = ['the the universe has very many stars',","             'the galaxy contains many stars',","             'the cold breeze of winter made it very cold outside']","             ","umg = Flask(__name__)","","","","","@umg.route(\"/\",methods=['POST','GET'])","def home():","    int_features=[int(x) for x in request.form.values()]","    final=[np.array(int_features)]","","    print (final)","","    # output='{0:.{1}f}'.format(prediction[0][1], 2)","","","    return render_template('home.html')","","    ","","if __name__ == '__main__':","    umg.run(host='0.0.0.0', port=5000, debug=True)","","","def dictOFTF_IDF():","dictOfWords = {}","","for index, sentence in enumerate(documents):","    tokenizedWords = sentence.split(' ')","    dictOfWords[index] = [(word,tokenizedWords.count(word)) for word in tokenizedWords]","","#print(dictOfWords)","","#second: remove duplicates","termFrequency = {}","","for i in range(0, len(documents)):","    listOfNoDuplicates = []","    for wordFreq in dictOfWords[i]:","        if wordFreq not in listOfNoDuplicates:","            listOfNoDuplicates.append(wordFreq)","        termFrequency[i] = listOfNoDuplicates","#print(termFrequency)","","#Third: normalized term frequency","normalizedTermFrequency = {}","for i in range(0, len(documents)):","    sentence = dictOfWords[i]","    lenOfSentence = len(sentence)","    listOfNormalized = []","    for wordFreq in termFrequency[i]:","        normalizedFreq = wordFreq[1]/lenOfSentence","        listOfNormalized.append((wordFreq[0],normalizedFreq))","    normalizedTermFrequency[i] = listOfNormalized","","#print(normalizedTermFrequency)","","","#---Calculate IDF","","#First: put al sentences together and tokenze words","","allDocuments = ''","for sentence in documents:","    allDocuments += sentence + ' '","allDocumentsTokenized = allDocuments.split(' ')","","#print(allDocumentsTokenized)","","allDocumentsNoDuplicates = []","","for word in allDocumentsTokenized:","    if word not in allDocumentsNoDuplicates:","        allDocumentsNoDuplicates.append(word)","","","#print(allDocumentsNoDuplicates)","","#Second calculate the number of documents where the term t appears","","dictOfNumberOfDocumentsWithTermInside = {}","","for index, voc in enumerate(allDocumentsNoDuplicates):","    count = 0","    for sentence in documents:","        if voc in sentence:","            count += 1","    dictOfNumberOfDocumentsWithTermInside[index] = (voc, count)","    ","    dictOFTF_IDF = {}","    for i in range(0,len(normalizedTermFrequency)):","        listOFTF_IDF = []","        TFsentence = normalizedTermFrequency[i]","        IDFsentence = dictOFIDFNoDuplicates[i]","    for x in range(0, len(TFsentence)):","        listOFTF_IDF.append((TFsentence[x][0],TFsentence[x][1]*IDFsentence[x][1]))","    dictOFTF_IDF[i] = listOFTF_IDF",""],"id":2},{"start":{"row":0,"column":0},"end":{"row":62,"column":63},"action":"insert","lines":["dictOfWords = {}","","for index, sentence in enumerate(documents):","    tokenizedWords = sentence.split(' ')","    dictOfWords[index] = [(word,tokenizedWords.count(word)) for word in tokenizedWords]","","#print(dictOfWords)","","#second: remove duplicates","termFrequency = {}","","for i in range(0, len(documents)):","    listOfNoDuplicates = []","    for wordFreq in dictOfWords[i]:","        if wordFreq not in listOfNoDuplicates:","            listOfNoDuplicates.append(wordFreq)","        termFrequency[i] = listOfNoDuplicates","#print(termFrequency)","","#Third: normalized term frequency","normalizedTermFrequency = {}","for i in range(0, len(documents)):","    sentence = dictOfWords[i]","    lenOfSentence = len(sentence)","    listOfNormalized = []","    for wordFreq in termFrequency[i]:","        normalizedFreq = wordFreq[1]/lenOfSentence","        listOfNormalized.append((wordFreq[0],normalizedFreq))","    normalizedTermFrequency[i] = listOfNormalized","","#print(normalizedTermFrequency)","","","#---Calculate IDF","","#First: put al sentences together and tokenze words","","allDocuments = ''","for sentence in documents:","    allDocuments += sentence + ' '","allDocumentsTokenized = allDocuments.split(' ')","","#print(allDocumentsTokenized)","","allDocumentsNoDuplicates = []","","for word in allDocumentsTokenized:","    if word not in allDocumentsNoDuplicates:","        allDocumentsNoDuplicates.append(word)","","","#print(allDocumentsNoDuplicates)","","#Second calculate the number of documents where the term t appears","","dictOfNumberOfDocumentsWithTermInside = {}","","for index, voc in enumerate(allDocumentsNoDuplicates):","    count = 0","    for sentence in documents:","        if voc in sentence:","            count += 1","    dictOfNumberOfDocumentsWithTermInside[index] = (voc, count)"]}],[{"start":{"row":0,"column":0},"end":{"row":62,"column":63},"action":"remove","lines":["dictOfWords = {}","","for index, sentence in enumerate(documents):","    tokenizedWords = sentence.split(' ')","    dictOfWords[index] = [(word,tokenizedWords.count(word)) for word in tokenizedWords]","","#print(dictOfWords)","","#second: remove duplicates","termFrequency = {}","","for i in range(0, len(documents)):","    listOfNoDuplicates = []","    for wordFreq in dictOfWords[i]:","        if wordFreq not in listOfNoDuplicates:","            listOfNoDuplicates.append(wordFreq)","        termFrequency[i] = listOfNoDuplicates","#print(termFrequency)","","#Third: normalized term frequency","normalizedTermFrequency = {}","for i in range(0, len(documents)):","    sentence = dictOfWords[i]","    lenOfSentence = len(sentence)","    listOfNormalized = []","    for wordFreq in termFrequency[i]:","        normalizedFreq = wordFreq[1]/lenOfSentence","        listOfNormalized.append((wordFreq[0],normalizedFreq))","    normalizedTermFrequency[i] = listOfNormalized","","#print(normalizedTermFrequency)","","","#---Calculate IDF","","#First: put al sentences together and tokenze words","","allDocuments = ''","for sentence in documents:","    allDocuments += sentence + ' '","allDocumentsTokenized = allDocuments.split(' ')","","#print(allDocumentsTokenized)","","allDocumentsNoDuplicates = []","","for word in allDocumentsTokenized:","    if word not in allDocumentsNoDuplicates:","        allDocumentsNoDuplicates.append(word)","","","#print(allDocumentsNoDuplicates)","","#Second calculate the number of documents where the term t appears","","dictOfNumberOfDocumentsWithTermInside = {}","","for index, voc in enumerate(allDocumentsNoDuplicates):","    count = 0","    for sentence in documents:","        if voc in sentence:","            count += 1","    dictOfNumberOfDocumentsWithTermInside[index] = (voc, count)"],"id":3},{"start":{"row":0,"column":0},"end":{"row":157,"column":0},"action":"insert","lines":["Skip to content","Search or jump to…","Pull requests","Issues","Marketplace","Explore"," ","@efrain-dev ","Simon-Fukada","/","PythonTF_IDFTutorial","Public","Code","Issues","Pull requests","Actions","Projects","Wiki","Security","Insights","PythonTF_IDFTutorial/tf_idf/main.py /","@Simon-Fukada","Simon-Fukada initial commit","Latest commit 3d76906 on 29 Dec 2018"," History"," 1 contributor","117 lines (81 sloc)  3.25 KB","   ","import math","","documents = ['the the universe has very many stars',","             'the galaxy contains many stars',","             'the cold breeze of winter made it very cold outside']","","#---Calculate term frequency --","","#First: tokenize words","dictOfWords = {}","","for index, sentence in enumerate(documents):","    tokenizedWords = sentence.split(' ')","    dictOfWords[index] = [(word,tokenizedWords.count(word)) for word in tokenizedWords]","","#print(dictOfWords)","","#second: remove duplicates","termFrequency = {}","","for i in range(0, len(documents)):","    listOfNoDuplicates = []","    for wordFreq in dictOfWords[i]:","        if wordFreq not in listOfNoDuplicates:","            listOfNoDuplicates.append(wordFreq)","        termFrequency[i] = listOfNoDuplicates","#print(termFrequency)","","#Third: normalized term frequency","normalizedTermFrequency = {}","for i in range(0, len(documents)):","    sentence = dictOfWords[i]","    lenOfSentence = len(sentence)","    listOfNormalized = []","    for wordFreq in termFrequency[i]:","        normalizedFreq = wordFreq[1]/lenOfSentence","        listOfNormalized.append((wordFreq[0],normalizedFreq))","    normalizedTermFrequency[i] = listOfNormalized","","#print(normalizedTermFrequency)","","","#---Calculate IDF","","#First: put al sentences together and tokenze words","","allDocuments = ''","for sentence in documents:","    allDocuments += sentence + ' '","allDocumentsTokenized = allDocuments.split(' ')","","#print(allDocumentsTokenized)","","allDocumentsNoDuplicates = []","","for word in allDocumentsTokenized:","    if word not in allDocumentsNoDuplicates:","        allDocumentsNoDuplicates.append(word)","","","#print(allDocumentsNoDuplicates)","","#Second calculate the number of documents where the term t appears","","dictOfNumberOfDocumentsWithTermInside = {}","","for index, voc in enumerate(allDocumentsNoDuplicates):","    count = 0","    for sentence in documents:","        if voc in sentence:","            count += 1","    dictOfNumberOfDocumentsWithTermInside[index] = (voc, count)","","#print(dictOfNumberOfDocumentsWithTermInside)","","","#calculate IDF","","dictOFIDFNoDuplicates = {}","","","for i in range(0, len(normalizedTermFrequency)):","    listOfIDFCalcs = []","    for word in normalizedTermFrequency[i]:","        for x in range(0, len(dictOfNumberOfDocumentsWithTermInside)):","            if word[0] == dictOfNumberOfDocumentsWithTermInside[x][0]:","                listOfIDFCalcs.append((word[0],math.log(len(documents)/dictOfNumberOfDocumentsWithTermInside[x][1])))","    dictOFIDFNoDuplicates[i] = listOfIDFCalcs","","#print(dictOFIDFNoDuplicates)","","#Multiply tf by idf for tf-idf","","dictOFTF_IDF = {}","for i in range(0,len(normalizedTermFrequency)):","    listOFTF_IDF = []","    TFsentence = normalizedTermFrequency[i]","    IDFsentence = dictOFIDFNoDuplicates[i]","    for x in range(0, len(TFsentence)):","        listOFTF_IDF.append((TFsentence[x][0],TFsentence[x][1]*IDFsentence[x][1]))","    dictOFTF_IDF[i] = listOFTF_IDF","","print(dictOFTF_IDF)","","from nltk.text import TextCollection","","mytexts = TextCollection(['the the universe has very many stars','the galaxy contains many stars','the cold breeze of winter made it very cold outside'])","","print(\"NLTK tf_idf\")","print(mytexts.tf_idf('very','the the universe has very many stars'))","","def test_tf(term, text):","    newText = text.split(' ')","    print(text.count(term))","    print(len(newText))","","#test_tf('universe','the the universe has very many stars')","© 2022 GitHub, Inc.","Terms","Privacy","Security","Status","Docs","Contact GitHub","Pricing","API","Training","Blog","About",""]}],[{"start":{"row":143,"column":0},"end":{"row":157,"column":0},"action":"remove","lines":["","#test_tf('universe','the the universe has very many stars')","© 2022 GitHub, Inc.","Terms","Privacy","Security","Status","Docs","Contact GitHub","Pricing","API","Training","Blog","About",""],"id":4}],[{"start":{"row":0,"column":0},"end":{"row":27,"column":3},"action":"remove","lines":["Skip to content","Search or jump to…","Pull requests","Issues","Marketplace","Explore"," ","@efrain-dev ","Simon-Fukada","/","PythonTF_IDFTutorial","Public","Code","Issues","Pull requests","Actions","Projects","Wiki","Security","Insights","PythonTF_IDFTutorial/tf_idf/main.py /","@Simon-Fukada","Simon-Fukada initial commit","Latest commit 3d76906 on 29 Dec 2018"," History"," 1 contributor","117 lines (81 sloc)  3.25 KB","   "],"id":5}],[{"start":{"row":105,"column":0},"end":{"row":116,"column":0},"action":"remove","lines":["from nltk.text import TextCollection","","mytexts = TextCollection(['the the universe has very many stars','the galaxy contains many stars','the cold breeze of winter made it very cold outside'])","","print(\"NLTK tf_idf\")","print(mytexts.tf_idf('very','the the universe has very many stars'))","","def test_tf(term, text):","    newText = text.split(' ')","    print(text.count(term))","    print(len(newText))",""],"id":6}],[{"start":{"row":105,"column":0},"end":{"row":106,"column":0},"action":"insert","lines":["",""],"id":7}],[{"start":{"row":106,"column":0},"end":{"row":106,"column":1},"action":"insert","lines":["d"],"id":8},{"start":{"row":106,"column":1},"end":{"row":106,"column":2},"action":"insert","lines":["e"]}],[{"start":{"row":106,"column":0},"end":{"row":106,"column":2},"action":"remove","lines":["de"],"id":9},{"start":{"row":106,"column":0},"end":{"row":106,"column":3},"action":"insert","lines":["def"]}],[{"start":{"row":106,"column":3},"end":{"row":106,"column":4},"action":"insert","lines":[" "],"id":10},{"start":{"row":106,"column":4},"end":{"row":106,"column":5},"action":"insert","lines":["r"]},{"start":{"row":106,"column":5},"end":{"row":106,"column":6},"action":"insert","lines":["e"]}],[{"start":{"row":106,"column":5},"end":{"row":106,"column":6},"action":"remove","lines":["e"],"id":11},{"start":{"row":106,"column":4},"end":{"row":106,"column":5},"action":"remove","lines":["r"]}],[{"start":{"row":106,"column":4},"end":{"row":106,"column":5},"action":"insert","lines":["r"],"id":12},{"start":{"row":106,"column":5},"end":{"row":106,"column":6},"action":"insert","lines":["e"]},{"start":{"row":106,"column":6},"end":{"row":106,"column":7},"action":"insert","lines":["s"]},{"start":{"row":106,"column":7},"end":{"row":106,"column":8},"action":"insert","lines":["u"]},{"start":{"row":106,"column":8},"end":{"row":106,"column":9},"action":"insert","lines":["t"]},{"start":{"row":106,"column":9},"end":{"row":106,"column":10},"action":"insert","lines":["l"]}],[{"start":{"row":106,"column":9},"end":{"row":106,"column":10},"action":"remove","lines":["l"],"id":13},{"start":{"row":106,"column":8},"end":{"row":106,"column":9},"action":"remove","lines":["t"]},{"start":{"row":106,"column":7},"end":{"row":106,"column":8},"action":"remove","lines":["u"]},{"start":{"row":106,"column":6},"end":{"row":106,"column":7},"action":"remove","lines":["s"]},{"start":{"row":106,"column":5},"end":{"row":106,"column":6},"action":"remove","lines":["e"]},{"start":{"row":106,"column":4},"end":{"row":106,"column":5},"action":"remove","lines":["r"]}],[{"start":{"row":106,"column":4},"end":{"row":106,"column":5},"action":"insert","lines":["r"],"id":14},{"start":{"row":106,"column":5},"end":{"row":106,"column":6},"action":"insert","lines":["e"]},{"start":{"row":106,"column":6},"end":{"row":106,"column":7},"action":"insert","lines":["s"]},{"start":{"row":106,"column":7},"end":{"row":106,"column":8},"action":"insert","lines":["u"]},{"start":{"row":106,"column":8},"end":{"row":106,"column":9},"action":"insert","lines":["t"]},{"start":{"row":106,"column":9},"end":{"row":106,"column":10},"action":"insert","lines":["l"]}],[{"start":{"row":106,"column":9},"end":{"row":106,"column":10},"action":"remove","lines":["l"],"id":15},{"start":{"row":106,"column":8},"end":{"row":106,"column":9},"action":"remove","lines":["t"]}],[{"start":{"row":106,"column":8},"end":{"row":106,"column":9},"action":"insert","lines":["l"],"id":16},{"start":{"row":106,"column":9},"end":{"row":106,"column":10},"action":"insert","lines":["t"]}],[{"start":{"row":106,"column":10},"end":{"row":106,"column":12},"action":"insert","lines":["()"],"id":17}],[{"start":{"row":106,"column":12},"end":{"row":106,"column":13},"action":"insert","lines":[":"],"id":18}],[{"start":{"row":106,"column":13},"end":{"row":106,"column":15},"action":"insert","lines":["\"\""],"id":19}],[{"start":{"row":106,"column":13},"end":{"row":106,"column":15},"action":"remove","lines":["\"\""],"id":20}],[{"start":{"row":106,"column":13},"end":{"row":107,"column":0},"action":"insert","lines":["",""],"id":21},{"start":{"row":107,"column":0},"end":{"row":107,"column":4},"action":"insert","lines":["    "]},{"start":{"row":107,"column":4},"end":{"row":107,"column":5},"action":"insert","lines":["r"]},{"start":{"row":107,"column":5},"end":{"row":107,"column":6},"action":"insert","lines":["e"]},{"start":{"row":107,"column":6},"end":{"row":107,"column":7},"action":"insert","lines":["t"]},{"start":{"row":107,"column":7},"end":{"row":107,"column":8},"action":"insert","lines":["u"]},{"start":{"row":107,"column":8},"end":{"row":107,"column":9},"action":"insert","lines":["r"]}],[{"start":{"row":107,"column":9},"end":{"row":107,"column":10},"action":"insert","lines":["n"],"id":22}],[{"start":{"row":107,"column":10},"end":{"row":107,"column":11},"action":"insert","lines":[" "],"id":23}],[{"start":{"row":106,"column":11},"end":{"row":106,"column":23},"action":"insert","lines":["dictOFTF_IDF"],"id":24}],[{"start":{"row":107,"column":11},"end":{"row":107,"column":23},"action":"insert","lines":["dictOFTF_IDF"],"id":25}],[{"start":{"row":103,"column":0},"end":{"row":104,"column":0},"action":"remove","lines":["print(dictOFTF_IDF)",""],"id":26}],[{"start":{"row":103,"column":0},"end":{"row":104,"column":0},"action":"remove","lines":["",""],"id":27}],[{"start":{"row":103,"column":0},"end":{"row":104,"column":0},"action":"insert","lines":["",""],"id":28},{"start":{"row":104,"column":0},"end":{"row":104,"column":1},"action":"insert","lines":["p"]},{"start":{"row":104,"column":1},"end":{"row":104,"column":2},"action":"insert","lines":["i"]}],[{"start":{"row":104,"column":1},"end":{"row":104,"column":2},"action":"remove","lines":["i"],"id":29}],[{"start":{"row":104,"column":1},"end":{"row":104,"column":2},"action":"insert","lines":["r"],"id":30},{"start":{"row":104,"column":2},"end":{"row":104,"column":3},"action":"insert","lines":["i"]},{"start":{"row":104,"column":3},"end":{"row":104,"column":4},"action":"insert","lines":["n"]},{"start":{"row":104,"column":4},"end":{"row":104,"column":5},"action":"insert","lines":["t"]}],[{"start":{"row":104,"column":5},"end":{"row":104,"column":7},"action":"insert","lines":["()"],"id":31}],[{"start":{"row":104,"column":6},"end":{"row":104,"column":18},"action":"insert","lines":["dictOFTF_IDF"],"id":32}],[{"start":{"row":104,"column":19},"end":{"row":105,"column":0},"action":"insert","lines":["",""],"id":33}],[{"start":{"row":107,"column":23},"end":{"row":108,"column":0},"action":"insert","lines":["",""],"id":34},{"start":{"row":108,"column":0},"end":{"row":108,"column":4},"action":"insert","lines":["    "]},{"start":{"row":108,"column":4},"end":{"row":109,"column":0},"action":"insert","lines":["",""]},{"start":{"row":109,"column":0},"end":{"row":109,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":109,"column":0},"end":{"row":109,"column":4},"action":"remove","lines":["    "],"id":35},{"start":{"row":108,"column":4},"end":{"row":109,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":108,"column":4},"end":{"row":109,"column":0},"action":"insert","lines":["",""],"id":36},{"start":{"row":109,"column":0},"end":{"row":109,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":109,"column":0},"end":{"row":109,"column":4},"action":"remove","lines":["    "],"id":37},{"start":{"row":108,"column":4},"end":{"row":109,"column":0},"action":"remove","lines":["",""]},{"start":{"row":108,"column":0},"end":{"row":108,"column":4},"action":"remove","lines":["    "]}],[{"start":{"row":108,"column":0},"end":{"row":109,"column":0},"action":"insert","lines":["",""],"id":38},{"start":{"row":109,"column":0},"end":{"row":110,"column":0},"action":"insert","lines":["",""]},{"start":{"row":110,"column":0},"end":{"row":110,"column":1},"action":"insert","lines":["d"]},{"start":{"row":110,"column":1},"end":{"row":110,"column":2},"action":"insert","lines":["d"]}],[{"start":{"row":110,"column":1},"end":{"row":110,"column":2},"action":"remove","lines":["d"],"id":39},{"start":{"row":110,"column":0},"end":{"row":110,"column":1},"action":"remove","lines":["d"]}],[{"start":{"row":110,"column":0},"end":{"row":110,"column":6},"action":"insert","lines":["result"],"id":40}],[{"start":{"row":106,"column":11},"end":{"row":106,"column":23},"action":"remove","lines":["dictOFTF_IDF"],"id":41}],[{"start":{"row":110,"column":6},"end":{"row":110,"column":8},"action":"insert","lines":["()"],"id":42}],[{"start":{"row":110,"column":0},"end":{"row":110,"column":1},"action":"insert","lines":[" "],"id":43}],[{"start":{"row":110,"column":0},"end":{"row":110,"column":1},"action":"insert","lines":["p"],"id":44},{"start":{"row":110,"column":1},"end":{"row":110,"column":2},"action":"insert","lines":["r"]},{"start":{"row":110,"column":2},"end":{"row":110,"column":3},"action":"insert","lines":["i"]},{"start":{"row":110,"column":3},"end":{"row":110,"column":4},"action":"insert","lines":["n"]},{"start":{"row":110,"column":4},"end":{"row":110,"column":5},"action":"insert","lines":["t"]}],[{"start":{"row":110,"column":5},"end":{"row":110,"column":7},"action":"insert","lines":["()"],"id":45}],[{"start":{"row":110,"column":8},"end":{"row":110,"column":16},"action":"remove","lines":["result()"],"id":46}],[{"start":{"row":110,"column":6},"end":{"row":110,"column":14},"action":"insert","lines":["result()"],"id":47}],[{"start":{"row":104,"column":0},"end":{"row":105,"column":0},"action":"remove","lines":["print(dictOFTF_IDF)",""],"id":48}],[{"start":{"row":109,"column":0},"end":{"row":109,"column":16},"action":"remove","lines":["print(result()) "],"id":49}],[{"start":{"row":0,"column":0},"end":{"row":109,"column":0},"action":"remove","lines":["","import math","","documents = ['the the universe has very many stars',","             'the galaxy contains many stars',","             'the cold breeze of winter made it very cold outside']","","#---Calculate term frequency --","","#First: tokenize words","dictOfWords = {}","","for index, sentence in enumerate(documents):","    tokenizedWords = sentence.split(' ')","    dictOfWords[index] = [(word,tokenizedWords.count(word)) for word in tokenizedWords]","","#print(dictOfWords)","","#second: remove duplicates","termFrequency = {}","","for i in range(0, len(documents)):","    listOfNoDuplicates = []","    for wordFreq in dictOfWords[i]:","        if wordFreq not in listOfNoDuplicates:","            listOfNoDuplicates.append(wordFreq)","        termFrequency[i] = listOfNoDuplicates","#print(termFrequency)","","#Third: normalized term frequency","normalizedTermFrequency = {}","for i in range(0, len(documents)):","    sentence = dictOfWords[i]","    lenOfSentence = len(sentence)","    listOfNormalized = []","    for wordFreq in termFrequency[i]:","        normalizedFreq = wordFreq[1]/lenOfSentence","        listOfNormalized.append((wordFreq[0],normalizedFreq))","    normalizedTermFrequency[i] = listOfNormalized","","#print(normalizedTermFrequency)","","","#---Calculate IDF","","#First: put al sentences together and tokenze words","","allDocuments = ''","for sentence in documents:","    allDocuments += sentence + ' '","allDocumentsTokenized = allDocuments.split(' ')","","#print(allDocumentsTokenized)","","allDocumentsNoDuplicates = []","","for word in allDocumentsTokenized:","    if word not in allDocumentsNoDuplicates:","        allDocumentsNoDuplicates.append(word)","","","#print(allDocumentsNoDuplicates)","","#Second calculate the number of documents where the term t appears","","dictOfNumberOfDocumentsWithTermInside = {}","","for index, voc in enumerate(allDocumentsNoDuplicates):","    count = 0","    for sentence in documents:","        if voc in sentence:","            count += 1","    dictOfNumberOfDocumentsWithTermInside[index] = (voc, count)","","#print(dictOfNumberOfDocumentsWithTermInside)","","","#calculate IDF","","dictOFIDFNoDuplicates = {}","","","for i in range(0, len(normalizedTermFrequency)):","    listOfIDFCalcs = []","    for word in normalizedTermFrequency[i]:","        for x in range(0, len(dictOfNumberOfDocumentsWithTermInside)):","            if word[0] == dictOfNumberOfDocumentsWithTermInside[x][0]:","                listOfIDFCalcs.append((word[0],math.log(len(documents)/dictOfNumberOfDocumentsWithTermInside[x][1])))","    dictOFIDFNoDuplicates[i] = listOfIDFCalcs","","#print(dictOFIDFNoDuplicates)","","#Multiply tf by idf for tf-idf","","dictOFTF_IDF = {}","for i in range(0,len(normalizedTermFrequency)):","    listOFTF_IDF = []","    TFsentence = normalizedTermFrequency[i]","    IDFsentence = dictOFIDFNoDuplicates[i]","    for x in range(0, len(TFsentence)):","        listOFTF_IDF.append((TFsentence[x][0],TFsentence[x][1]*IDFsentence[x][1]))","    dictOFTF_IDF[i] = listOFTF_IDF","","","","def result():","    return dictOFTF_IDF","","",""],"id":50},{"start":{"row":0,"column":0},"end":{"row":85,"column":0},"action":"insert","lines":["","import math","","documentos = ['the the universe has very many stars',","             'the galaxy contains many stars',","             'the cold breeze of winter made it very cold outside']","","","dictadoDePalabras = {}","","for indice, sentencia in enumerate(documentos):","    tokenPalabra = sentencia.split(' ')","    dictadoDePalabras[indice] = [(palabra,tokenPalabra.count(palabra)) for palabra in tokenPalabra]","","termFrecuencia = {}","","for i in range(0, len(documentos)):","    listaDeNoDuplicados = []","    for palabraFrequencia in dictadoDePalabras[i]:","        if palabraFrequencia not in listaDeNoDuplicados:","            listaDeNoDuplicados.append(palabraFrequencia)","        termFrecuencia[i] = listaDeNoDuplicados","","terminoNormalizadoFrecuencia = {}","for i in range(0, len(documentos)):","    sentencia = dictadoDePalabras[i]","    lenDeSentencia = len(sentencia)","    listaDeNormalizados = []","    for palabraFrequencia in termFrecuencia[i]:","        normalizedFreq = palabraFrequencia[1]/lenDeSentencia","        listaDeNormalizados.append((palabraFrequencia[0],normalizedFreq))","    terminoNormalizadoFrecuencia[i] = listaDeNormalizados","","","","todosDocumento = ''","for sentencia in documentos:","    todosDocumento += sentencia + ' '","todosDocumentosToken = todosDocumento.split(' ')","","","todosDocumentoNoDuplicados = []","","for palabra in todosDocumentosToken:","    if palabra not in todosDocumentoNoDuplicados:","        todosDocumentoNoDuplicados.append(palabra)","","","","dictDeNumDeDocConTermDentro = {}","","for indice, voc in enumerate(todosDocumentoNoDuplicados):","    count = 0","    for sentencia in documentos:","        if voc in sentencia:","            count += 1","    dictDeNumDeDocConTermDentro[indice] = (voc, count)","","","","dictOFIDFNoDuplicados = {}","","","for i in range(0, len(terminoNormalizadoFrecuencia)):","    listOfIDFCalcs = []","    for palabra in terminoNormalizadoFrecuencia[i]:","        for x in range(0, len(dictDeNumDeDocConTermDentro)):","            if palabra[0] == dictDeNumDeDocConTermDentro[x][0]:","                listOfIDFCalcs.append((palabra[0],math.log(len(documentos)/dictDeNumDeDocConTermDentro[x][1])))","    dictOFIDFNoDuplicados[i] = listOfIDFCalcs","","","dictOFTF_IDF = {}","for i in range(0,len(terminoNormalizadoFrecuencia)):","    listaOFTF_IDF = []","    TFsentencia = terminoNormalizadoFrecuencia[i]","    IDFsentencia = dictOFIDFNoDuplicados[i]","    for x in range(0, len(TFsentencia)):","        listaOFTF_IDF.append((TFsentencia[x][0],TFsentencia[x][1]*IDFsentencia[x][1]))","    dictOFTF_IDF[i] = listaOFTF_IDF","","","","def result():","    return dictOFTF_IDF",""]}],[{"start":{"row":3,"column":15},"end":{"row":3,"column":51},"action":"remove","lines":["the the universe has very many stars"],"id":51},{"start":{"row":3,"column":15},"end":{"row":3,"column":117},"action":"insert","lines":["Administración Avanzada de Bases de Datos es un curso en el que se busca llevar al estudiante más allá"]}],[{"start":{"row":4,"column":14},"end":{"row":4,"column":44},"action":"remove","lines":["the galaxy contains many stars"],"id":52},{"start":{"row":4,"column":14},"end":{"row":4,"column":116},"action":"insert","lines":["del paradigma de base de datos relacional y SQL que se enseña tradicionalmente en el nivel de pregrado"]}],[{"start":{"row":5,"column":14},"end":{"row":5,"column":65},"action":"remove","lines":["the cold breeze of winter made it very cold outside"],"id":54},{"start":{"row":5,"column":14},"end":{"row":5,"column":112},"action":"insert","lines":["en las Escuelas de Ingeniería. En este curso se pretende introducir al estudiante en temas de gran"]}],[{"start":{"row":5,"column":112},"end":{"row":5,"column":113},"action":"insert","lines":[","],"id":55}],[{"start":{"row":5,"column":113},"end":{"row":6,"column":0},"action":"insert","lines":["",""],"id":56},{"start":{"row":6,"column":0},"end":{"row":6,"column":13},"action":"insert","lines":["             "]}],[{"start":{"row":6,"column":12},"end":{"row":6,"column":13},"action":"remove","lines":[" "],"id":57},{"start":{"row":6,"column":8},"end":{"row":6,"column":12},"action":"remove","lines":["    "]},{"start":{"row":6,"column":4},"end":{"row":6,"column":8},"action":"remove","lines":["    "]},{"start":{"row":6,"column":0},"end":{"row":6,"column":4},"action":"remove","lines":["    "]}],[{"start":{"row":5,"column":113},"end":{"row":6,"column":0},"action":"remove","lines":["",""],"id":58},{"start":{"row":5,"column":112},"end":{"row":5,"column":113},"action":"remove","lines":[","]}],[{"start":{"row":5,"column":113},"end":{"row":5,"column":114},"action":"insert","lines":[","],"id":59}],[{"start":{"row":5,"column":114},"end":{"row":6,"column":0},"action":"insert","lines":["",""],"id":60},{"start":{"row":6,"column":0},"end":{"row":6,"column":13},"action":"insert","lines":["             "]},{"start":{"row":6,"column":13},"end":{"row":7,"column":0},"action":"insert","lines":["",""]},{"start":{"row":7,"column":0},"end":{"row":7,"column":13},"action":"insert","lines":["             "]}],[{"start":{"row":6,"column":13},"end":{"row":7,"column":0},"action":"insert","lines":["             'en las Escuelas de Ingeniería. En este curso se pretende introducir al estudiante en temas de gran',",""],"id":61}],[{"start":{"row":6,"column":27},"end":{"row":6,"column":125},"action":"remove","lines":["en las Escuelas de Ingeniería. En este curso se pretende introducir al estudiante en temas de gran"],"id":62},{"start":{"row":6,"column":27},"end":{"row":6,"column":122},"action":"insert","lines":["importancia en el ámbito de las bases de datos como desempeño y escalabilidad. Además, se busca"]}],[{"start":{"row":6,"column":13},"end":{"row":6,"column":26},"action":"remove","lines":["             "],"id":63}],[{"start":{"row":7,"column":0},"end":{"row":8,"column":0},"action":"insert","lines":["             'importancia en el ámbito de las bases de datos como desempeño y escalabilidad. Además, se busca',",""],"id":64}],[{"start":{"row":7,"column":14},"end":{"row":7,"column":109},"action":"remove","lines":["importancia en el ámbito de las bases de datos como desempeño y escalabilidad. Además, se busca"],"id":65},{"start":{"row":7,"column":14},"end":{"row":7,"column":115},"action":"insert","lines":["presentar al estudiante paradigmas contemporáneos y complementarios del modelo relacional, como bases"]}],[{"start":{"row":8,"column":0},"end":{"row":9,"column":0},"action":"insert","lines":["             'presentar al estudiante paradigmas contemporáneos y complementarios del modelo relacional, como bases',",""],"id":66}],[{"start":{"row":8,"column":14},"end":{"row":8,"column":115},"action":"remove","lines":["presentar al estudiante paradigmas contemporáneos y complementarios del modelo relacional, como bases"],"id":67},{"start":{"row":8,"column":14},"end":{"row":8,"column":63},"action":"insert","lines":["de datos multidimensionales, geográficas y NoSQL."]}],[{"start":{"row":8,"column":65},"end":{"row":9,"column":0},"action":"remove","lines":["",""],"id":68}],[{"start":{"row":8,"column":62},"end":{"row":8,"column":63},"action":"remove","lines":["."],"id":69}],[{"start":{"row":5,"column":43},"end":{"row":5,"column":44},"action":"remove","lines":["."],"id":70}],[{"start":{"row":7,"column":103},"end":{"row":7,"column":104},"action":"remove","lines":[","],"id":71}],[{"start":{"row":6,"column":99},"end":{"row":6,"column":100},"action":"remove","lines":[","],"id":72}],[{"start":{"row":6,"column":91},"end":{"row":6,"column":92},"action":"remove","lines":["."],"id":73}]]},"ace":{"folds":[],"scrolltop":660,"scrollleft":0,"selection":{"start":{"row":14,"column":47},"end":{"row":14,"column":47},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":46,"state":"start","mode":"ace/mode/python"}},"timestamp":1647058821729,"hash":"a0e804c3c471a29f0fdc21deccc35c47e8aaf1a3"}